{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "# import torchvision\n",
    "import torchvision.transforms.functional as fn\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import os, os.path\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restructure images and prepare training, validation and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when running the below cell again with diff dimension and saving files to xray folder, run this before\n",
    "# !rmdir /s /q xray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by inspection of files, min Width = 384, min Height = 127\n",
    "def process_images(crop, dimension,zeroOne):\n",
    "    imgs = torch.zeros((1,torch.prod(dimension)+1))\n",
    "    train, validation, test = imgs,imgs,imgs\n",
    "\n",
    "    in_directory = [\"chest_xray/test/NORMAL\",\"chest_xray/test/PNEUMONIA\",\\\n",
    "                \"chest_xray/val/NORMAL\",\"chest_xray/val/PNEUMONIA\",\\\n",
    "                \"chest_xray/train/NORMAL\",\"chest_xray/train/PNEUMONIA\"]\n",
    "\n",
    "    out_directory = [\"xray/test/NORMAL\",\"xray/test/PNEUMONIA\",\\\n",
    "                \"xray/val/NORMAL\",\"xray/val/PNEUMONIA\",\\\n",
    "                \"xray/train/NORMAL\",\"xray/train/PNEUMONIA\"]\n",
    "\n",
    "    valid_images = [\".jpeg\",\".jpg\",\".png\"]\n",
    "\n",
    "    for i in range(len(in_directory)):\n",
    "        if not os.path.exists(out_directory[i]):\n",
    "            # os.makedirs(out_directory[i]) #if you are saving to the xray folder\n",
    "            imgs = torch.zeros((1,torch.prod(dimension)+1))\n",
    "            group = in_directory[i].split(\"/\")[1:]\n",
    "            if (group[-1] == \"NORMAL\"):\n",
    "                label = 1.\n",
    "            else:\n",
    "                label = -1.\n",
    "            label = torch.tensor(label).reshape((1,1))\n",
    "\n",
    "            for f in os.listdir(in_directory[i]):\n",
    "                ext = os.path.splitext(f)[1]\n",
    "                if ext.lower() not in valid_images:\n",
    "                    continue\n",
    "                img = Image.open(os.path.join(in_directory[i],f)).convert(\"L\")\n",
    "                \n",
    "                if crop:\n",
    "                    img = fn.center_crop(img, output_size=list(np.array(dimension))) #crop\n",
    "                else:\n",
    "                    img = fn.resize(img, size=list(np.array(dimension))) # resize\n",
    "\n",
    "                # img.save(f\"{out_directory[i]}/{f}\") # optional: save image to folder\n",
    "                img = torch.tensor(np.array(img)).reshape((1,torch.prod(dimension)))\n",
    "                imgs = torch.vstack((imgs,torch.hstack((img,label))))\n",
    "\n",
    "            imgs = imgs[1:,:]\n",
    "            if (group[0] == \"train\"):\n",
    "                train = torch.vstack((train,imgs))\n",
    "            elif (group[0] == \"val\"):\n",
    "                validation = torch.vstack((validation,imgs))\n",
    "            else:\n",
    "                test = torch.vstack((test,imgs))\n",
    "\n",
    "    train, validation, test = train[1:,:], validation[1:,:],test[1:,:]\n",
    "    train[:,:-1] /= 255\n",
    "    validation[:,:-1] /= 255\n",
    "    test[:,:-1] /= 255\n",
    "    if zeroOne:\n",
    "        train[:,:-1] = (train[:,:-1]>=0.5).long()\n",
    "        validation[:,:-1] = (validation[:,:-1]>=0.5).long()\n",
    "        test[:,:-1] = (test[:,:-1]>=0.5).long()\n",
    "    # remove if you shuffle elsewhere\n",
    "    train=train[torch.randperm(train.shape[0])]\n",
    "    validation = validation[torch.randperm(validation.shape[0])]\n",
    "    test = test[torch.randperm(test.shape[0])]\n",
    "    return train, validation, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = torch.tensor([128,128])\n",
    "train, validation, test = process_images(crop=0, dimension=dimension, zeroOne=False) # crop = 0 -> resize, crop=1 -> crop. w is square image dimension\n",
    "train.shape, validation.shape, test.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Naive Bayes weak classifier with Adaptive Boositng (AdaBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_linear(array, new_min, new_max):\n",
    "    \"\"\"Rescale an arrary linearly.\"\"\"\n",
    "    minimum, maximum = torch.min(array), torch.max(array)\n",
    "    m = (new_max - new_min) / (maximum - minimum)\n",
    "    b = new_min - m * minimum\n",
    "    return m * array + b\n",
    "\n",
    "def bayespost(data,px,py):\n",
    "    # we need to incorporate the prior probability p(y) since p(y|x) is\n",
    "    # proportional to p(x|y) p(y)\n",
    "    data = data.reshape((len(data),1))\n",
    "    logpx = torch.log(px)\n",
    "    logpxneg = torch.log(1-px)\n",
    "    logpy = torch.log(py)\n",
    "    logpost = logpy.clone()\n",
    "    logpost += (logpx * data).sum(0) + logpxneg * (1-data)\n",
    "    # normalize to prevent overflow or underflow by subtracting the largest value\n",
    "    logpost -= torch.max(logpost)\n",
    "    # and compute the softmax using logpx\n",
    "    post = torch.exps(logpost)\n",
    "    post /= torch.sum(post)\n",
    "    return post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pure naive classifier (for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train[:,:-1], train[:,-1]\n",
    "idx1 = (y_train==1.)\n",
    "idx0 = ~idx1\n",
    "ycount = torch.ones((2))\n",
    "ycount[0],ycount[1] = idx1.sum(), idx0.sum()\n",
    "py = ycount / ycount.sum()\n",
    "\n",
    "for gauss in [True,False]:\n",
    "    if gauss:\n",
    "        print(\"Gaussian Naive Bayes\")\n",
    "        ## For continous [0-1] pixel images (Gaussian)\n",
    "        means = torch.ones((torch.prod(dimension),2))\n",
    "        vars, px = torch.ones((torch.prod(dimension),2)), torch.ones((torch.prod(dimension),2))\n",
    "        means[:,0] = X_train[idx1,:].mean(axis=0)\n",
    "        means[:,1] = X_train[idx0,:].mean(axis=0)\n",
    "        vars[:,0] = X_train[idx1,:].var(axis=0)\n",
    "        vars[:,1] = X_train[idx0,:].var(axis=0)\n",
    "        # px[:,0] = MultivariateNormal(means[:,0],torch.diag(vars[:,0])).sample()\n",
    "        # px[:,1] = MultivariateNormal(means[:,1],torch.diag(vars[:,1])).sample()\n",
    "        # px = rescale_linear(px, 1e-5, 1-1e-3)\n",
    "\n",
    "        #train\n",
    "        x1 = (torch.log(1/torch.sqrt(2*3.1415*vars[:,0]))\\\n",
    "                -0.5*((train[:,:-1] - means[:,0])**2)/vars[:,0]).sum(1)\n",
    "        x0 = (torch.log(1/torch.sqrt(2*3.1415*vars[:,1]))\\\n",
    "                -0.5*((train[:,:-1] - means[:,1])**2)/vars[:,1]).sum(1)\n",
    "        logpy = torch.log(py)\n",
    "        x = torch.vstack((x1,x0)).T\n",
    "        pred = (x+logpy).argmax(1)\n",
    "        pred[pred==1] = -1.\n",
    "        pred[pred==0] = 1.\n",
    "        err_rate = (train[:,-1] != pred).sum()/len(pred)\n",
    "        print(f\"Training Misclassification rate: {err_rate:.{4}}\")\n",
    "        print(f\"Training Accuracy: {100*(1-err_rate):.{4}}%\")\n",
    "\n",
    "        #validation\n",
    "        x1 = (torch.log(1/torch.sqrt(2*3.1415*vars[:,0]))\\\n",
    "                -0.5*((validation[:,:-1] - means[:,0])**2)/vars[:,0]).sum(1)\n",
    "        x0 = (torch.log(1/torch.sqrt(2*3.1415*vars[:,1]))\\\n",
    "                -0.5*((validation[:,:-1] - means[:,1])**2)/vars[:,1]).sum(1)\n",
    "        logpy = torch.log(py)\n",
    "        x = torch.vstack((x1,x0)).T\n",
    "        pred = (x+logpy).argmax(1)\n",
    "        pred[pred==1] = -1.\n",
    "        pred[pred==0] = 1.\n",
    "        err_rate = (validation[:,-1] != pred).sum()/len(pred)\n",
    "        print(f\"\\nValidation Misclassification rate: {err_rate:.{4}}\")\n",
    "        print(f\"Validation Accuracy: {100*(1-err_rate):.{4}}%\")\n",
    "\n",
    "        #test\n",
    "        x1 = (torch.log(1/torch.sqrt(2*3.1415*vars[:,0]))\\\n",
    "                -0.5*((test[:,:-1] - means[:,0])**2)/vars[:,0]).sum(1)\n",
    "        x0 = (torch.log(1/torch.sqrt(2*3.1415*vars[:,1]))\\\n",
    "                -0.5*((test[:,:-1] - means[:,1])**2)/vars[:,1]).sum(1)\n",
    "        logpy = torch.log(py)\n",
    "        x = torch.vstack((x1,x0)).T\n",
    "        pred = (x+logpy).argmax(1)\n",
    "        pred[pred==1] = -1.\n",
    "        pred[pred==0] = 1.\n",
    "        err_rate = (test[:,-1] != pred).sum()/len(pred)\n",
    "        print(f\"\\nTest Misclassification rate: {err_rate:.{4}}\")\n",
    "        print(f\"Test Accuracy: {100*(1-err_rate):.{4}}%\")\n",
    "        \n",
    "    else:\n",
    "        print(\"*****************************\\nMultinomial Naive Bayes\")\n",
    "        # For 0 to 1 pixel images (treating it as Multinomial)\n",
    "        xcount = torch.ones((torch.prod(dimension),2)) #Laplace smoothing\n",
    "        xcount[:,0] += X_train[idx1,:].sum(axis=0)\n",
    "        xcount[:,1] += X_train[idx0,:].sum(axis=0)\n",
    "        px = (xcount / ycount.reshape(1,2)) #broadcasting\n",
    "\n",
    "        # train\n",
    "        # t_pred = torch.tensor(np.apply_along_axis(bayespost, 1, train[:,:-1],px,py))\n",
    "        t_pred = torch.ones((len(train),2))\n",
    "        for i in range(len(train)):\n",
    "            t_pred[i,:] = bayespost(train[i,:-1], px, py)\n",
    "        pred_label = (t_pred[:,0] > t_pred[:,1]).long()\n",
    "        pred_label[pred_label == 1] = 1.\n",
    "        pred_label[pred_label == 0] = -1.\n",
    "        err_rate = (train[:,-1] != pred_label).sum()/len(pred_label)\n",
    "        print(f\"Training Misclassification rate: {err_rate:.{4}}\")\n",
    "        print(f\"Training Accuracy: {100*(1-err_rate):.{4}}%\")\n",
    "\n",
    "        # validation\n",
    "        # v_pred = torch.tensor(np.apply_along_axis(bayespost, 1, validation[:,:-1],px,py))\n",
    "        v_pred = torch.ones((len(validation),2))\n",
    "        for i in range(len(validation)):\n",
    "            v_pred[i,:] = bayespost(validation[i,:-1], px, py)\n",
    "        pred_label = (v_pred[:,0] > v_pred[:,1]).long()\n",
    "        pred_label[pred_label == 1] = 1.\n",
    "        pred_label[pred_label == 0] = -1.\n",
    "        err_rate = (validation[:,-1] != pred_label).sum()/len(pred_label)\n",
    "        print(f\"\\nValidation Misclassification rate: {err_rate:.{4}}\")\n",
    "        print(f\"Validation Accuracy: {100*(1-err_rate):.{4}}%\")\n",
    "\n",
    "        # test\n",
    "        # test_pred = torch.tensor(np.apply_along_axis(bayespost, 1, test[:,:-1],px,py))\n",
    "        test_pred = torch.ones((len(test),2))\n",
    "        for i in range(len(test)):\n",
    "            test_pred[i,:] = bayespost(test[i,:-1], px, py)\n",
    "        pred_label = (test_pred[:,0] > test_pred[:,1]).long()\n",
    "        pred_label[pred_label == 1] = 1.\n",
    "        pred_label[pred_label == 0] = -1.\n",
    "        err_rate = (test[:,-1] != pred_label).sum()/len(pred_label)\n",
    "        print(f\"\\nTest Misclassification rate: {err_rate:.{4}}\")\n",
    "        print(f\"Test Accuracy: {100*(1-err_rate):.{4}}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to scikit\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "nb = GaussianNB()\n",
    "nb.fit(train[:,:-1], train[:,-1])\n",
    "print(\"Gaussian Naive Bayes\")\n",
    "print(\"Train Naive Bayes accuracy: %.2f%%\" %(100*nb.score(train[:,:-1], train[:,-1])))\n",
    "print(\"Validation Naive Bayes accuracy: %.2f%%\" %(100*nb.score(validation[:,:-1], validation[:,-1])))\n",
    "print(\"Test Naive Bayes accuracy: %.2f%%\" %(100*nb.score(test[:,:-1], test[:,-1])))\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(train[:,:-1], train[:,-1])\n",
    "print(\"\\nMultinomial Naive Bayes\")\n",
    "print(\"Train Naive Bayes accuracy: %.2f%%\" %(100*nb.score(train[:,:-1], train[:,-1])))\n",
    "print(\"Validation Naive Bayes accuracy: %.2f%%\" %(100*nb.score(validation[:,:-1], validation[:,-1])))\n",
    "print(\"Test Naive Bayes accuracy: %.2f%%\" %(100*nb.score(test[:,:-1], test[:,-1])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "\n",
    "    # initialization\n",
    "    def __init__(self):\n",
    "\n",
    "        self.num_classes = 2\n",
    "        self.max_iter = 0\n",
    "        self.prob = torch.tensor(0.)\n",
    "        self.probs = torch.tensor(0.)\n",
    "        self.alphas = torch.tensor(0.)\n",
    "        self.pred_labels = torch.tensor(0.)\n",
    "\n",
    "\n",
    "    def fit(self,data,dimension,gauss,max_iter = 100):\n",
    "\n",
    "        self.alphas = torch.tensor(0.) \n",
    "        self.pred_labels = torch.ones((len(data),1))\n",
    "        self.gauss = gauss\n",
    "        self.prob = 1/len(data)*torch.ones((len(data),1)) #initialize weights to 1/N\n",
    "        self.probs = torch.ones((len(data),1))\n",
    "        self.probs = torch.hstack((self.probs,self.prob))\n",
    "        self.max_iter = max_iter\n",
    "        self.dimension = torch.prod(dimension)\n",
    "        self.py = torch.ones((self.num_classes,self.max_iter))\n",
    "        self.px = torch.ones((self.dimension,self.num_classes,self.max_iter))\n",
    "        self.means = torch.ones((torch.prod(dimension),self.num_classes,self.max_iter))\n",
    "        self.vars = torch.ones((torch.prod(dimension),self.num_classes,self.max_iter))\n",
    "\n",
    "        for ctr in range(max_iter):\n",
    "            # get samples of training data with replacement and find px, py\n",
    "            # torch.seed()\n",
    "            idx = self.probs[:,-1].multinomial(num_samples=len(data), replacement=True)\n",
    "            X_train, y_train = data[idx,:-1], data[idx,-1]\n",
    "            idx1 = (y_train==1.)\n",
    "            idx0 = ~idx1\n",
    "\n",
    "            ycount = torch.ones(self.num_classes)\n",
    "            ycount[0],ycount[1] = idx1.sum(), idx0.sum()\n",
    "            self.py[:,ctr] = ycount / ycount.sum()\n",
    "\n",
    "            if self.gauss:\n",
    "                ## For continous [0-1] pixel images (Gaussian)\n",
    "                self.means[:,0,ctr] = X_train[idx1,:].mean(axis=0)\n",
    "                self.means[:,1,ctr] = X_train[idx0,:].mean(axis=0)\n",
    "                self.vars[:,0,ctr] = X_train[idx1,:].var(axis=0)\n",
    "                self.vars[:,1,ctr] = X_train[idx0,:].var(axis=0)\n",
    "                # self.px[:,0,ctr] = MultivariateNormal(means[:,0,ctr],torch.diag(vars[:,0,ctr])).sample()\n",
    "                # self.px[:,1,ctr] = MultivariateNormal(means[:,1,ctr],torch.diag(vars[:,1,ctr])).sample()\n",
    "                # self.px[:,:,ctr] = rescale_linear(self.px[:,:,ctr], 1e-5, 1-1e-3)\n",
    "                x1 = (torch.log(1/torch.sqrt(2*3.1415*self.vars[:,0,ctr]))\\\n",
    "                        -0.5*((X_train - self.means[:,0,ctr])**2)/self.vars[:,0,ctr]).sum(1)\n",
    "                x0 = (torch.log(1/torch.sqrt(2*3.1415*self.vars[:,1,ctr]))\\\n",
    "                        -0.5*((X_train - self.means[:,1,ctr])**2)/self.vars[:,1,ctr]).sum(1)\n",
    "                logpy = torch.log(self.py[:,ctr])\n",
    "                x = torch.vstack((x1,x0)).T\n",
    "                self.pred_label = (x+logpy).argmax(1)\n",
    "                self.pred_label[self.pred_label==1] = -1.\n",
    "                self.pred_label[self.pred_label==0] = 1.\n",
    "\n",
    "                # scikit\n",
    "                # self.nb = GaussianNB()\n",
    "                # self.nb.fit(X_train, y_train)\n",
    "                # self.pred_label = torch.tensor(self.nb.predict(X_train))\n",
    "            else:\n",
    "                ## For 0 to 1 pixel images (treating each fraction as Multinomial data)\n",
    "                xcount = torch.ones(self.dimension,self.num_classes) # Laplace smoothing\n",
    "                xcount[:,0] += X_train[idx1,:].sum(axis=0)\n",
    "                xcount[:,1] += X_train[idx0,:].sum(axis=0)\n",
    "                self.px[:,:,ctr] = (xcount / ycount.reshape(1,self.num_classes))#broadcasting\n",
    "                # get predictions on the whole training dataset\n",
    "                # features = data[:,:-1]\n",
    "                # pred = torch.tensor(np.apply_along_axis(bayespost, 1, features,self.px[:,:,ctr],self.py[:,ctr]))\n",
    "                pred = torch.ones((len(data),self.num_classes))\n",
    "                for i in range(len(data)):\n",
    "                    pred[i,:] = bayespost(data[i,:-1],self.px[:,:,ctr],self.py[:,ctr])\n",
    "                self.pred_label = (pred[:,0] > pred[:,1]).long()\n",
    "                self.pred_label[self.pred_label == 1] = 1.\n",
    "                self.pred_label[self.pred_label == 0] = -1.\n",
    "\n",
    "                # scikit\n",
    "                # self.nb = MultinomialNB()\n",
    "                # self.nb.fit(X_train, y_train)\n",
    "                # self.pred_label = torch.tensor(self.nb.predict(X_train))\n",
    "\n",
    "\n",
    "            # estimate misclassification from predictions\n",
    "            err_idx = (data[:,-1] != self.pred_label)\n",
    "            err = (err_idx*self.probs[:,-1]).sum() + 1e-12        \n",
    "            if err >= 1 - 1 / self.num_classes:\n",
    "                print(\"Worse than random guess. Stopped Boosting\")\n",
    "                break\n",
    "\n",
    "            # measure performance of the naive bayes with alpha\n",
    "            # If the error (err) is 0.5, then the performance of the naive bayes will be zero.\n",
    "            # If the error is 0 or 1, then the performance will become infinity or -infinity respectively.\n",
    "            # Large alpha means classifier is a good one\n",
    "            alpha = 0.5*torch.log((1-err)/err)\n",
    "            self.alphas = torch.hstack((self.alphas,alpha)) # store\n",
    "            self.pred_labels = torch.hstack((self.pred_labels,self.pred_label.reshape((len(data),1))))\n",
    "            # increase weights of the wrongly classified records and decrease weights of the correctly classified records\n",
    "            # training label * prediction label = -1 -> misclassified -> e^(-alpha) is large -> weight increased\n",
    "            # training label * prediction label = 1 -> correctly classified -> e^(-alpha) is small -> weight decreased\n",
    "            prob = self.probs[:,-1]*torch.exp(-alpha*data[:,-1]*self.pred_label)\n",
    "            if prob.sum() <= 0:\n",
    "                print(\"prob sum invalid\")\n",
    "                break   \n",
    "            prob /= prob.sum() # normalize weights\n",
    "            if ctr++1 >= max_iter: #err <= tol:\n",
    "                break\n",
    "            self.probs = torch.hstack((self.probs,prob.reshape((len(train),1)))) # store  \n",
    "        # remove garbage initialization values\n",
    "        self.alphas = self.alphas[1:]\n",
    "        self.probs = self.probs[:,1:]\n",
    "        self.pred_labels = self.pred_labels[:,1:]\n",
    "\n",
    "        # adaboost = final classifier is the sign of the performance weighted sum of predicted values at different iterations\n",
    "        self.final_pred = torch.sign((self.alphas*self.pred_labels).sum(1))\n",
    "        err_rate = (data[:,-1] != self.final_pred).sum()/len(self.final_pred)\n",
    "        print(f\"Training Misclassification rate: {err_rate:.{4}}\")\n",
    "        print(f\"Training Accuracy: {100*(1-err_rate):.{4}}%\")\n",
    "    \n",
    "\n",
    "    def predict(self,data):\n",
    "\n",
    "        pred_labels = torch.ones((len(data),1))\n",
    "\n",
    "        for ctr in range(len(self.alphas)):\n",
    "            if self.gauss:\n",
    "                x1 = (torch.log(1/torch.sqrt(2*3.1415*self.vars[:,0,ctr]))\\\n",
    "                        -0.5*((data[:,:-1] - self.means[:,0,ctr])**2)/self.vars[:,0,ctr]).sum(1)\n",
    "                x0 = (torch.log(1/torch.sqrt(2*3.1415*self.vars[:,1,ctr]))\\\n",
    "                        -0.5*((data[:,:-1] - self.means[:,1,ctr])**2)/self.vars[:,1,ctr]).sum(1)\n",
    "                logpy = torch.log(self.py[:,ctr])\n",
    "                x = torch.vstack((x1,x0)).T\n",
    "                pred_label = (x+logpy).argmax(1)\n",
    "                pred_label[pred_label==1] = -1.\n",
    "                pred_label[pred_label==0] = 1.\n",
    "\n",
    "                # pred_label = torch.tensor(self.nb.predict(data[:,:-1]))\n",
    "            else:\n",
    "                # features = data[:,:-1]\n",
    "                # pred = torch.tensor(np.apply_along_axis(bayespost, 1, features,self.px[:,:,ctr],self.py[:,ctr]))\n",
    "                self.pred = torch.ones((len(data),self.num_classes))\n",
    "                for i in range(len(data)):\n",
    "                    self.pred[i,:] = bayespost(data[i,:-1],self.px[:,:,ctr],self.py[:,ctr])\n",
    "                pred_label = (self.pred[:,0] > self.pred[:,1]).long()\n",
    "                pred_label[pred_label == 1] = 1.\n",
    "                pred_label[pred_label == 0] = -1.\n",
    "\n",
    "                # pred_label = torch.tensor(self.nb.predict(data[:,:-1]))\n",
    "                \n",
    "            pred_labels = torch.hstack((pred_labels,pred_label.reshape((len(data),1))))\n",
    "        \n",
    "        pred_labels = pred_labels[:,1:]\n",
    "        final_pred = torch.sign((self.alphas*pred_labels).sum(1))\n",
    "        return final_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M=100\n",
    "for gauss in [True, False]:\n",
    "    # train\n",
    "    if gauss:\n",
    "        print(\"Gaussian Naive Bayes\")\n",
    "    else:\n",
    "        print(\"*****************************\\nMultinomial Naive Bayes\")\n",
    "    \n",
    "    AB = AdaBoost()\n",
    "    AB.fit(train,dimension,gauss,max_iter=M)\n",
    "\n",
    "    # validate\n",
    "    val_pred = AB.predict(validation)\n",
    "    err_rate = (validation[:,-1] != val_pred).sum()/len(val_pred)\n",
    "    print(f\"\\nValidation Misclassification rate: {err_rate:.{4}}\")\n",
    "    print(f\"Validation Accuracy: {100*(1-err_rate):.{4}}%\")\n",
    "\n",
    "    # test\n",
    "    test_pred = AB.predict(test)\n",
    "    err_rate = (test[:,-1] != test_pred).sum()/len(test_pred)\n",
    "    print(f\"\\nTest Misclassification rate: {err_rate:.{4}}\")\n",
    "    print(f\"Test Accuracy: {100*(1-err_rate):.{4}}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with scikit AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# train\n",
    "clf = AdaBoostClassifier(n_estimators=M, random_state=0)\n",
    "clf.fit(train[:,:-1], train[:,-1])\n",
    "t_acc = 100*clf.score(train[:,:-1], train[:,-1])\n",
    "\n",
    "# validate\n",
    "v_p = clf.predict(validation[:,:-1])\n",
    "v_acc = 100*clf.score(validation[:,:-1], validation[:,-1])\n",
    "\n",
    "# test\n",
    "t_p = clf.predict(test[:,:-1])\n",
    "te_acc = 100*clf.score(test[:,:-1], test[:,-1])\n",
    "\n",
    "print(\"Training Accuracy: %.2f%%, Validation Accuracy: %.2f%%, Testing Accuracy %.2f%%\" % (t_acc, v_acc, te_acc))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f515e17ece7fdd10b748da9c779c75bf809ff89faf3460578638f0110370c271"
  },
  "kernelspec": {
   "display_name": "Python 3.6.15 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
